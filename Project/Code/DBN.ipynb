{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbkOsejox1YF"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9taF8cTxq-2",
        "outputId": "8778c8b2-3b08-4017-fbca-dedd2af8f8e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "miRNA's Row and Column Number :  (368, 672)\n",
            "Copy Number Variation's Row and Column Number :  (19568, 672)\n",
            "DNA_Methylation's Row and Column Number :  (19049, 672)\n",
            "mRNA's Row and Column Number :  (18206, 672)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read omic data from each dataset\n",
        "miRNA = pd.read_csv('drive/MyDrive/BRCA_data/BRCA_miRNA.csv')\n",
        "cnv = pd.read_csv('drive/MyDrive/BRCA_data/BRCA_Copy Nunber Variation.csv')\n",
        "methy = pd.read_csv('drive/MyDrive/BRCA_data/BRCA_DNA_Methylation.csv')\n",
        "mRNA = pd.read_csv('drive/MyDrive/BRCA_data/BRCA_mRNA.csv')\n",
        "\n",
        "# Get the row and column number of each dataset\n",
        "print(\"miRNA's Row and Column Number : \", miRNA.shape)\n",
        "print(\"Copy Number Variation's Row and Column Number : \", cnv.shape)\n",
        "print(\"DNA_Methylation's Row and Column Number : \", methy.shape)\n",
        "print(\"mRNA's Row and Column Number : \", mRNA.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu-tPotryJva"
      },
      "outputs": [],
      "source": [
        "# View the data type of features and sample_ID in miRNA dataset\n",
        "print(\"miRNA's Data Type\\n \", miRNA.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diL-Itq4yNIx"
      },
      "outputs": [],
      "source": [
        "# View the data type of features and sample_ID in cnv dataset\n",
        "print(\"CNV's Data Type\\n \",cnv.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHf7e8emyOnK"
      },
      "outputs": [],
      "source": [
        "# View the data type of features and sample_ID in dna_methy dataset\n",
        "print(\"DNA Methylation's Data Type\\n \",methy.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZBmc_upyQcL"
      },
      "outputs": [],
      "source": [
        "# View the data type of features and sample_ID in mRNA dataset\n",
        "print(\"mRNA's Data Type\\n \",mRNA.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdFYyQ4tyWxV"
      },
      "outputs": [],
      "source": [
        "# Transpose the row (features) to column\n",
        "miRNA_T = miRNA.T\n",
        "cnv_T = cnv.T\n",
        "methy_T = methy.T\n",
        "mRNA_T = mRNA.T\n",
        "\n",
        "# View transposed result\n",
        "miRNA_T.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv_sRe1NyYUF"
      },
      "outputs": [],
      "source": [
        "# Drop the feature name row\n",
        "miRNA_dropped=miRNA_T.iloc[1:]\n",
        "cnv_dropped=cnv_T.iloc[1:]\n",
        "methy_dropped=methy_T.iloc[1:]\n",
        "mRNA_dropped=mRNA_T.iloc[1:]\n",
        "\n",
        "# View result\n",
        "miRNA_dropped.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBUl7_RyyZ9E"
      },
      "outputs": [],
      "source": [
        "# Index naming\n",
        "miRNA_dropped.index.name='Sample'\n",
        "cnv_dropped.index.name='Sample'\n",
        "methy_dropped.index.name='Sample'\n",
        "mRNA_dropped.index.name='Sample'\n",
        "\n",
        "# View result\n",
        "miRNA_dropped.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpfftlJWybzQ"
      },
      "outputs": [],
      "source": [
        "# Obtain total number of missing values in each dataset\n",
        "print('Number of miRNA missing values: ', miRNA_dropped.isnull().sum().sum())\n",
        "print('Number of CNV missing values: ', cnv_dropped.isnull().sum().sum())\n",
        "print('Number of DNA Methylation  missing values: ', methy_dropped.isnull().sum().sum())\n",
        "print('Number of mRNA missing values: ', mRNA_dropped.isnull().sum().sum())\n",
        "\n",
        "# Obtain total number of duplicates in each dataset\n",
        "print('\\nmiRNA duplicates number: ', miRNA_dropped.duplicated().sum())\n",
        "print('CNV duplicates number: ', cnv_dropped.duplicated().sum())\n",
        "print('DNA Methylation duplicates number: ', methy_dropped.duplicated().sum())\n",
        "print('mRNA duplicates number: ', mRNA_dropped.duplicated().sum())\n",
        "\n",
        "# Print to csv files\n",
        "miRNA_dropped.to_csv('drive/MyDrive/BRCA_result/post_miRNA.csv')\n",
        "cnv_dropped.to_csv('drive/MyDrive/BRCA_result/post_cnv.csv')\n",
        "methy_dropped.to_csv('drive/MyDrive/BRCA_result/post_methy.csv')\n",
        "mRNA_dropped.to_csv('drive/MyDrive/BRCA_result/post_mRNA.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwEoH7m-ydzf"
      },
      "source": [
        "# **Feature Selection using SVM-RFE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dXX53IcymzU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "miRNA_data = pd.read_csv('drive/MyDrive/BRCA_result/post_miRNA.csv', header=0, index_col=None)\n",
        "cnv_data = pd.read_csv('drive/MyDrive/BRCA_result/post_cnv.csv', header=0, index_col=None)\n",
        "methy_data = pd.read_csv('drive/MyDrive/BRCA_result/post_methy.csv', header=0, index_col=None)\n",
        "mRNA_data = pd.read_csv('drive/MyDrive/BRCA_result/post_mRNA.csv', header=0, index_col=None)\n",
        "\n",
        "# Sort sample arrangement\n",
        "miRNA_data.sort_values(by='Sample', ascending=True, inplace=True)\n",
        "cnv_data.sort_values(by='Sample', ascending=True, inplace=True)\n",
        "methy_data.sort_values(by='Sample', ascending=True, inplace=True)\n",
        "mRNA_data .sort_values(by='Sample', ascending=True, inplace=True)\n",
        "\n",
        "# Get name of sample\n",
        "sample_name = miRNA_data['Sample'].tolist()\n",
        "\n",
        "# Get target value, y from sample class dataset\n",
        "sample_label = pd.read_csv('drive/MyDrive/BRCA_data/BRCA_label.csv',header=0,index_col=None)\n",
        "\n",
        "# Change label string to numerical value\n",
        "label_mapping ={'LumA': 0, 'LumB': 1, 'Basal': 2, 'Her2': 3, 'Normal': 4}\n",
        "sample_label['Label'] = sample_label['Label'].replace(label_mapping)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ebKqCtLzVYo"
      },
      "source": [
        "## **1st Variation: miRNA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tpdpe6pzYVB",
        "outputId": "70976cde-a0cf-49e5-95a7-5a358208385f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features Importance: \n",
            "     Columns    Weight\n",
            "0         0 -0.017636\n",
            "1         1  0.126673\n",
            "2         2 -0.092692\n",
            "3         3  0.024360\n",
            "4         4 -0.095524\n",
            "..      ...       ...\n",
            "363     363  0.031034\n",
            "364     364  0.025525\n",
            "365     365 -0.087108\n",
            "366     366 -0.028983\n",
            "367     367  0.060615\n",
            "\n",
            "[368 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Get X and Y values\n",
        "X_omics1 = miRNA_data.iloc[:, 1:]\n",
        "Y_omics1 =  sample_label.iloc[:, 0]\n",
        "\n",
        "# Initialize an SVM model with a linear kernel\n",
        "estimator = SVR(kernel='linear')\n",
        "\n",
        "# Get the feature importance or weight\n",
        "estimator.fit(X_omics1, Y_omics1)\n",
        "features_importance = pd.DataFrame({'Columns': X_omics1.columns, 'Weight':estimator.coef_.flatten()})\n",
        "print(\"Features Importance: \\n\",features_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "eqmrIKPBzcEd",
        "outputId": "e9ca74f5-c4c7-40fd-a078-f23ee7adbfc6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=SVR(kernel=&#x27;linear&#x27;), n_features_to_select=250, step=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFE</label><div class=\"sk-toggleable__content\"><pre>RFE(estimator=SVR(kernel=&#x27;linear&#x27;), n_features_to_select=250, step=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "RFE(estimator=SVR(kernel='linear'), n_features_to_select=250, step=10)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply RFE to select the top 250 features\n",
        "selector = RFE(estimator,n_features_to_select=250, step=10)\n",
        "\n",
        "# Train model\n",
        "selector.fit( X_omics1,Y_omics1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOr3tC49zf2E",
        "outputId": "0d77323e-2eb8-4ee0-ec93-2edd3e9345cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Selected Features: \n",
            "     Columns  Selected\n",
            "0         0      True\n",
            "1         1      True\n",
            "2         2     False\n",
            "3         3      True\n",
            "4         4      True\n",
            "..      ...       ...\n",
            "363     363      True\n",
            "364     364     False\n",
            "365     365      True\n",
            "366     366     False\n",
            "367     367      True\n",
            "\n",
            "[368 rows x 2 columns]\n",
            "\n",
            "Features Ranking: \n",
            "     Columns  Ranking\n",
            "0         0        1\n",
            "1         1        1\n",
            "2         2        4\n",
            "3         3        1\n",
            "4         4        1\n",
            "..      ...      ...\n",
            "363     363        1\n",
            "364     364        3\n",
            "365     365        1\n",
            "366     366        5\n",
            "367     367        1\n",
            "\n",
            "[368 rows x 2 columns]\n",
            "\n",
            "Unselected Features: \n",
            " Index(['2', '8', '11', '12', '23', '24', '26', '28', '32', '34', '42', '43',\n",
            "       '48', '50', '67', '69', '70', '72', '79', '80', '95', '99', '100',\n",
            "       '104', '108', '117', '118', '122', '128', '132', '133', '146', '152',\n",
            "       '155', '167', '171', '176', '189', '190', '191', '199', '202', '203',\n",
            "       '205', '211', '221', '244', '247', '255', '259', '260', '273', '279',\n",
            "       '281', '316', '317', '318', '326', '327', '330', '333', '337', '338',\n",
            "       '341', '345', '360', '364', '366'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        " # Get selected features list\n",
        "features_selected = pd.DataFrame({'Columns':X_omics1.columns, 'Selected':selector.support_})\n",
        "print(\"\\nSelected Features: \\n\",features_selected)\n",
        "\n",
        "# Get features ranking list\n",
        "features_rank = pd.DataFrame({'Columns': X_omics1.columns, 'Ranking': selector.ranking_})\n",
        "print(\"\\nFeatures Ranking: \\n\",features_rank)\n",
        "\n",
        "# Get unselected features list\n",
        "features_unselected = X_omics1.columns[np.logical_not(selector.get_support())]\n",
        "print(\"\\nUnselected Features: \\n\", features_unselected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7aXTLuSzhnq",
        "outputId": "b56b3f9e-6eaf-42f4-990a-456066ba7891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SVM-RFE Model Performance based on miRNA Data\n",
            "Coefficient of determination (R^2):  0.6877122202770727\n"
          ]
        }
      ],
      "source": [
        "# Test and evaluate model\n",
        "print(\"\\nSVM-RFE Model Performance based on miRNA Data\")\n",
        "print(\"Coefficient of determination (R^2): \",selector.score(X_omics1,Y_omics1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnpWx9WOzi6m",
        "outputId": "e88f44e1-ea8f-4c73-c26e-849694ccd0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "selected feature from miRNA\n",
            "\n",
            "              Sample         0         1         4         5         6  \\\n",
            "0    TCGA.3C.AAAU.01  0.068317  0.068932 -1.656853 -0.038283  0.501125   \n",
            "1    TCGA.3C.AALI.01 -0.301684 -0.318009 -0.715963  0.460975 -1.999304   \n",
            "2    TCGA.3C.AALJ.01 -0.150810 -0.122747 -0.971038  0.866585  2.074809   \n",
            "3    TCGA.3C.AALK.01  0.107831  0.097594  0.711952 -0.454282  0.227441   \n",
            "4    TCGA.5L.AAT0.01  0.395211  0.412879  0.426323 -1.545556 -0.952282   \n",
            "..               ...       ...       ...       ...       ...       ...   \n",
            "666  TCGA.WT.AB44.01  0.511958  0.504240  1.028977 -0.254668  0.528134   \n",
            "667  TCGA.XX.A899.01  1.225298  1.219548  1.186182 -1.238797  0.879213   \n",
            "668  TCGA.XX.A89A.01  0.667662  0.666012  1.534247  0.305716  1.276369   \n",
            "669  TCGA.Z7.A8R5.01 -0.211878 -0.210525  1.070328 -0.716526 -0.598473   \n",
            "670  TCGA.Z7.A8R6.01  0.474240  0.488117 -0.155655  1.384395  1.237552   \n",
            "\n",
            "            7         9        10        13  ...       351       352  \\\n",
            "0   -2.390084 -0.760042 -2.453567  0.164738  ...  1.710640  1.717982   \n",
            "1   -0.659788 -1.050266 -0.901638 -0.810861  ...  1.022632  1.016292   \n",
            "2    1.080746  0.523204 -0.434303 -1.045474  ...  1.257298  1.252362   \n",
            "3   -0.735552 -1.015715 -0.067349  0.798193  ... -0.271230 -0.264533   \n",
            "4   -0.280966 -0.818574 -0.670743  0.344608  ... -0.737147 -0.711763   \n",
            "..        ...       ...       ...       ...  ...       ...       ...   \n",
            "666  0.757211  0.378499 -0.081399  0.300618  ... -1.046794 -0.974745   \n",
            "667  1.455206  0.921697  0.192758  1.342510  ...  0.070190  0.045684   \n",
            "668  1.253312  0.556762  0.188198  1.099337  ... -0.385313 -0.370678   \n",
            "669 -0.229774 -0.465549  1.345377  1.132516  ... -0.886547 -0.860318   \n",
            "670  0.851405  0.409391 -0.512685  0.078713  ... -0.775653 -0.828494   \n",
            "\n",
            "          353       354       355       356       357       359       365  \\\n",
            "0    1.720551  0.229904  0.220471  1.393365 -0.160793  0.108421 -0.143572   \n",
            "1    1.019438 -1.052846 -1.084899 -0.146885 -0.049663  0.569264  1.103662   \n",
            "2    1.245308 -0.515728 -0.532584 -0.091650  1.232475  1.952380  1.940594   \n",
            "3   -0.288654  0.012655  0.046348  1.077402 -0.220505  0.119549 -0.831098   \n",
            "4   -0.758717 -0.258087 -0.222046 -0.107654  0.296995 -0.248950 -0.885905   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "666 -1.052610 -0.601973 -0.612437  1.146529 -0.532331 -0.345687  0.034551   \n",
            "667  0.002930  0.990850  0.935746  0.080636  0.347012 -0.066436  0.419871   \n",
            "668 -0.384373 -0.369404 -0.513460 -0.683967  0.096259 -0.896460  0.277618   \n",
            "669 -0.865821 -0.082323 -0.061231 -1.093552 -0.928749 -1.035048 -0.187117   \n",
            "670 -0.821256  0.899118  0.919183  1.573963  1.929109  1.714101  0.914841   \n",
            "\n",
            "          367  \n",
            "0    1.603022  \n",
            "1   -1.506825  \n",
            "2    0.787948  \n",
            "3    0.030739  \n",
            "4   -0.645458  \n",
            "..        ...  \n",
            "666 -0.893453  \n",
            "667 -0.898431  \n",
            "668 -0.363449  \n",
            "669 -0.572713  \n",
            "670  0.859040  \n",
            "\n",
            "[671 rows x 251 columns]\n"
          ]
        }
      ],
      "source": [
        "# Put selected features in dataframe with sample name\n",
        "selected_features1 = X_omics1.iloc[:, selector.support_]\n",
        "pd_selected_features1 = pd.DataFrame(selected_features1)\n",
        "pd_selected_features1.insert(0, 'Sample', sample_name)\n",
        "print(\"\\nselected feature from miRNA\\n\")\n",
        "print(pd_selected_features1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U6BslhCzkTi",
        "outputId": "8ffd9024-6310-416a-e4ec-0a8e83366b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success! Features Selection results can be seen in result folder.\n"
          ]
        }
      ],
      "source": [
        "# Output merged result into a  CSV file\n",
        "pd_selected_features1.to_csv('drive/MyDrive/BRCA_result/miRNA_data_250.csv', header=True, index=False)\n",
        "print('Success! Features Selection results can be seen in result folder.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aM9Qpywce0S"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2-uRuHc1oe",
        "outputId": "eb86db68-9b91-45f4-fad4-5fe41b4fccc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-belief-network'...\n",
            "remote: Enumerating objects: 798, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 798 (delta 13), reused 20 (delta 9), pack-reused 763\u001b[K\n",
            "Receiving objects: 100% (798/798), 183.45 KiB | 2.18 MiB/s, done.\n",
            "Resolving deltas: 100% (459/459), done.\n",
            "/content/deep-belief-network\n",
            "Processing /content/deep-belief-network\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.16.4 (from deep-belief-network==1.0.5)\n",
            "  Downloading numpy-1.16.4.zip (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==0.18.1 (from deep-belief-network==1.0.5)\n",
            "  Downloading scipy-0.18.1.zip (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==0.18.1 (from deep-belief-network==1.0.5)\n",
            "  Downloading scikit-learn-0.18.1.tar.gz (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of deep-belief-network to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.5.0 (from deep-belief-network) (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.5.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install the package from GitHub\n",
        "!git clone https://github.com/albertbup/deep-belief-network.git\n",
        "%cd deep-belief-network\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL4kjAbtc542"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from dbn import SupervisedDBNClassification\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Get the absolute path to the file\n",
        "file_path = os.path.abspath('/content/deep-belief-network/BRCA_result/miRNA_data_250.csv')\n",
        "labelFile_path = os.path.abspath('/content/deep-belief-network/BRCA_data/BRCA_label.csv')\n",
        "\n",
        "# Load dataset\n",
        "miRNA_data = pd.read_csv(file_path, header=0, index_col=None)\n",
        "\n",
        "# Get target value, y from sample class dataset\n",
        "sample_label = pd.read_csv(labelFile_path,header=0,index_col=None)\n",
        "\n",
        "# Change label string to numerical value\n",
        "label_mapping ={'LumA': 0, 'LumB': 1, 'Basal': 2, 'Her2': 3, 'Normal': 4}\n",
        "sample_label['Label'] = sample_label['Label'].replace(label_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p328TNpEfN_T"
      },
      "source": [
        "## **1st Variation: miRNA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM6gHZlldJi9"
      },
      "outputs": [],
      "source": [
        "# Get X and Y values\n",
        "X = miRNA_data.iloc[:, 1:]\n",
        "Y  =  sample_label.iloc[:, 0]\n",
        "\n",
        "# Normalize the input data to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3KiA1V9c3l5"
      },
      "source": [
        "### **Without SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "13mddRmsdPKQ",
        "outputId": "b19329df-080d-4e4a-990c-a1148fabbcd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] Pre-training step:\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 6.313261\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 5.964528\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 6.120016\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 6.326709\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 6.420840\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 6.877525\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 6.168845\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 5.913792\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 5.855211\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 6.442810\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 3.291603\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 3.047093\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 2.627090\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 4.722574\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 1.936215\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 3.168108\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 4.618691\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 4.268474\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 2.263192\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 4.548624\n",
            "[END] Pre-training step\n",
            "[START] Fine tuning step:\n",
            ">> Epoch 1 finished \tANN training loss 7.166177\n",
            ">> Epoch 2 finished \tANN training loss 6.704716\n",
            ">> Epoch 3 finished \tANN training loss 6.568752\n",
            ">> Epoch 4 finished \tANN training loss 6.444322\n",
            ">> Epoch 5 finished \tANN training loss 6.429312\n",
            ">> Epoch 6 finished \tANN training loss 6.281084\n",
            ">> Epoch 7 finished \tANN training loss 6.090972\n",
            ">> Epoch 8 finished \tANN training loss 5.666247\n",
            ">> Epoch 9 finished \tANN training loss 5.372749\n",
            ">> Epoch 10 finished \tANN training loss 5.114357\n",
            ">> Epoch 11 finished \tANN training loss 5.165901\n",
            ">> Epoch 12 finished \tANN training loss 4.855756\n",
            ">> Epoch 13 finished \tANN training loss 4.995500\n",
            ">> Epoch 14 finished \tANN training loss 4.429867\n",
            ">> Epoch 15 finished \tANN training loss 4.436631\n",
            ">> Epoch 16 finished \tANN training loss 4.414924\n",
            ">> Epoch 17 finished \tANN training loss 4.475865\n",
            ">> Epoch 18 finished \tANN training loss 4.463856\n",
            ">> Epoch 19 finished \tANN training loss 4.285599\n",
            ">> Epoch 20 finished \tANN training loss 4.020276\n",
            ">> Epoch 21 finished \tANN training loss 4.069855\n",
            ">> Epoch 22 finished \tANN training loss 4.343010\n",
            ">> Epoch 23 finished \tANN training loss 3.939912\n",
            ">> Epoch 24 finished \tANN training loss 4.177687\n",
            ">> Epoch 25 finished \tANN training loss 3.959351\n",
            ">> Epoch 26 finished \tANN training loss 3.945768\n",
            ">> Epoch 27 finished \tANN training loss 3.686542\n",
            ">> Epoch 28 finished \tANN training loss 3.652265\n",
            ">> Epoch 29 finished \tANN training loss 3.761854\n",
            ">> Epoch 30 finished \tANN training loss 3.797201\n",
            ">> Epoch 31 finished \tANN training loss 3.550299\n",
            ">> Epoch 32 finished \tANN training loss 3.516429\n",
            ">> Epoch 33 finished \tANN training loss 3.563646\n",
            ">> Epoch 34 finished \tANN training loss 3.751523\n",
            ">> Epoch 35 finished \tANN training loss 3.531904\n",
            ">> Epoch 36 finished \tANN training loss 3.524830\n",
            ">> Epoch 37 finished \tANN training loss 3.495044\n",
            ">> Epoch 38 finished \tANN training loss 3.366941\n",
            ">> Epoch 39 finished \tANN training loss 3.429930\n",
            ">> Epoch 40 finished \tANN training loss 3.592829\n",
            ">> Epoch 41 finished \tANN training loss 3.486600\n",
            ">> Epoch 42 finished \tANN training loss 3.332999\n",
            ">> Epoch 43 finished \tANN training loss 3.317767\n",
            ">> Epoch 44 finished \tANN training loss 3.473452\n",
            ">> Epoch 45 finished \tANN training loss 3.680039\n",
            ">> Epoch 46 finished \tANN training loss 3.424912\n",
            ">> Epoch 47 finished \tANN training loss 3.218842\n",
            ">> Epoch 48 finished \tANN training loss 3.272404\n",
            ">> Epoch 49 finished \tANN training loss 3.413933\n",
            ">> Epoch 50 finished \tANN training loss 3.292203\n",
            ">> Epoch 51 finished \tANN training loss 3.343791\n",
            ">> Epoch 52 finished \tANN training loss 3.150756\n",
            ">> Epoch 53 finished \tANN training loss 3.129259\n",
            ">> Epoch 54 finished \tANN training loss 3.173489\n",
            ">> Epoch 55 finished \tANN training loss 3.253204\n",
            ">> Epoch 56 finished \tANN training loss 3.276358\n",
            ">> Epoch 57 finished \tANN training loss 2.906191\n",
            ">> Epoch 58 finished \tANN training loss 3.349686\n",
            ">> Epoch 59 finished \tANN training loss 3.262333\n",
            ">> Epoch 60 finished \tANN training loss 3.214928\n",
            ">> Epoch 61 finished \tANN training loss 3.121882\n",
            ">> Epoch 62 finished \tANN training loss 3.123327\n",
            ">> Epoch 63 finished \tANN training loss 3.108572\n",
            ">> Epoch 64 finished \tANN training loss 2.920868\n",
            ">> Epoch 65 finished \tANN training loss 2.980252\n",
            ">> Epoch 66 finished \tANN training loss 2.915171\n",
            ">> Epoch 67 finished \tANN training loss 2.955232\n",
            ">> Epoch 68 finished \tANN training loss 3.187611\n",
            ">> Epoch 69 finished \tANN training loss 3.034090\n",
            ">> Epoch 70 finished \tANN training loss 2.993380\n",
            ">> Epoch 71 finished \tANN training loss 2.955838\n",
            ">> Epoch 72 finished \tANN training loss 2.887693\n",
            ">> Epoch 73 finished \tANN training loss 2.826369\n",
            ">> Epoch 74 finished \tANN training loss 2.995170\n",
            ">> Epoch 75 finished \tANN training loss 2.839553\n",
            ">> Epoch 76 finished \tANN training loss 2.679260\n",
            ">> Epoch 77 finished \tANN training loss 2.864101\n",
            ">> Epoch 78 finished \tANN training loss 2.953214\n",
            ">> Epoch 79 finished \tANN training loss 3.062076\n",
            ">> Epoch 80 finished \tANN training loss 2.767930\n",
            ">> Epoch 81 finished \tANN training loss 2.887775\n",
            ">> Epoch 82 finished \tANN training loss 3.043200\n",
            ">> Epoch 83 finished \tANN training loss 2.854694\n",
            ">> Epoch 84 finished \tANN training loss 2.647414\n",
            ">> Epoch 85 finished \tANN training loss 3.039173\n",
            ">> Epoch 86 finished \tANN training loss 2.980842\n",
            ">> Epoch 87 finished \tANN training loss 2.574121\n",
            ">> Epoch 88 finished \tANN training loss 3.077817\n",
            ">> Epoch 89 finished \tANN training loss 2.523564\n",
            ">> Epoch 90 finished \tANN training loss 2.909146\n",
            ">> Epoch 91 finished \tANN training loss 2.868141\n",
            ">> Epoch 92 finished \tANN training loss 2.517467\n",
            ">> Epoch 93 finished \tANN training loss 2.886709\n",
            ">> Epoch 94 finished \tANN training loss 2.800728\n",
            ">> Epoch 95 finished \tANN training loss 2.682015\n",
            ">> Epoch 96 finished \tANN training loss 2.882720\n",
            ">> Epoch 97 finished \tANN training loss 2.874316\n",
            ">> Epoch 98 finished \tANN training loss 2.777254\n",
            ">> Epoch 99 finished \tANN training loss 2.878169\n",
            ">> Epoch 100 finished \tANN training loss 2.811298\n",
            "[END] Fine tuning step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SupervisedDBNClassification()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SupervisedDBNClassification</label><div class=\"sk-toggleable__content\"><pre>SupervisedDBNClassification()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SupervisedDBNClassification()"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train model\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.01,    # Reduced learning rate\n",
        "                                         learning_rate=0.1,            # Reduced learning rate\n",
        "                                         n_epochs_rbm=10,\n",
        "                                         n_iter_backprop=100,\n",
        "                                         batch_size=32,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "\n",
        "# Fit model\n",
        "classifier.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf9ItFWBdY0c",
        "outputId": "153ed634-fc1c-4f04-e73a-abfb7a2c2729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Accuracy: 0.822222\n"
          ]
        }
      ],
      "source": [
        "# Test model\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP3-gAP2diOs"
      },
      "source": [
        "### **SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNfKXv_zdldp",
        "outputId": "8f4f5b51-684b-4468-85a2-9d29c466d264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of resampled data: (1385, 250)\n",
            "Shape of Y: (1385,)\n"
          ]
        }
      ],
      "source": [
        "# Apply SMOTE on training data\n",
        "sm= SMOTE(k_neighbors=1,random_state=0)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, Y_train)\n",
        "\n",
        "print ('Shape of resampled data: {}'.format(X_train_res.shape))\n",
        "print ('Shape of Y: {}'.format(y_train_res.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugyzwfkud6oC"
      },
      "outputs": [],
      "source": [
        "# Data visualization before SMOTE\n",
        "counter = Counter(Y_train)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/deep-belief-network/BRCA_result/static/images/preSMOTE_miRNA.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gFUNyE-d-1N"
      },
      "outputs": [],
      "source": [
        "# Data visualization after SMOTE\n",
        "counter = Counter(y_train_res)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/deep-belief-network/BRCA_result/static/images/SMOTE_miRNA.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rk7xA_PxeHNC",
        "outputId": "4d577246-ca5a-4968-ddf1-9a089d1f2a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] Pre-training step:\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 5.690391\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 6.459843\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 6.838501\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 6.284727\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 5.724720\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 5.127762\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 5.236968\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 4.913271\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 5.649900\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 5.284998\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 1.277229\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 2.025713\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.486369\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 1.683530\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 1.558864\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 1.517237\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 1.230018\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.929287\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.848667\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 1.785840\n",
            "[END] Pre-training step\n",
            "[START] Fine tuning step:\n",
            ">> Epoch 1 finished \tANN training loss 7.556810\n",
            ">> Epoch 2 finished \tANN training loss 6.664554\n",
            ">> Epoch 3 finished \tANN training loss 6.033396\n",
            ">> Epoch 4 finished \tANN training loss 5.471345\n",
            ">> Epoch 5 finished \tANN training loss 4.799513\n",
            ">> Epoch 6 finished \tANN training loss 4.456692\n",
            ">> Epoch 7 finished \tANN training loss 4.288843\n",
            ">> Epoch 8 finished \tANN training loss 4.293844\n",
            ">> Epoch 9 finished \tANN training loss 3.768019\n",
            ">> Epoch 10 finished \tANN training loss 3.632277\n",
            ">> Epoch 11 finished \tANN training loss 3.491318\n",
            ">> Epoch 12 finished \tANN training loss 3.201268\n",
            ">> Epoch 13 finished \tANN training loss 3.127001\n",
            ">> Epoch 14 finished \tANN training loss 2.923071\n",
            ">> Epoch 15 finished \tANN training loss 2.678134\n",
            ">> Epoch 16 finished \tANN training loss 3.104786\n",
            ">> Epoch 17 finished \tANN training loss 2.808732\n",
            ">> Epoch 18 finished \tANN training loss 2.932066\n",
            ">> Epoch 19 finished \tANN training loss 2.822436\n",
            ">> Epoch 20 finished \tANN training loss 2.629729\n",
            ">> Epoch 21 finished \tANN training loss 2.688630\n",
            ">> Epoch 22 finished \tANN training loss 2.563318\n",
            ">> Epoch 23 finished \tANN training loss 2.475132\n",
            ">> Epoch 24 finished \tANN training loss 2.798058\n",
            ">> Epoch 25 finished \tANN training loss 2.738210\n",
            ">> Epoch 26 finished \tANN training loss 2.617688\n",
            ">> Epoch 27 finished \tANN training loss 2.620852\n",
            ">> Epoch 28 finished \tANN training loss 2.525088\n",
            ">> Epoch 29 finished \tANN training loss 2.474365\n",
            ">> Epoch 30 finished \tANN training loss 2.584760\n",
            ">> Epoch 31 finished \tANN training loss 2.507723\n",
            ">> Epoch 32 finished \tANN training loss 2.535416\n",
            ">> Epoch 33 finished \tANN training loss 2.643139\n",
            ">> Epoch 34 finished \tANN training loss 2.513612\n",
            ">> Epoch 35 finished \tANN training loss 2.457771\n",
            ">> Epoch 36 finished \tANN training loss 2.466861\n",
            ">> Epoch 37 finished \tANN training loss 2.378355\n",
            ">> Epoch 38 finished \tANN training loss 2.328637\n",
            ">> Epoch 39 finished \tANN training loss 2.499907\n",
            ">> Epoch 40 finished \tANN training loss 2.403520\n",
            ">> Epoch 41 finished \tANN training loss 2.386218\n",
            ">> Epoch 42 finished \tANN training loss 2.302821\n",
            ">> Epoch 43 finished \tANN training loss 2.428430\n",
            ">> Epoch 44 finished \tANN training loss 2.331206\n",
            ">> Epoch 45 finished \tANN training loss 2.400141\n",
            ">> Epoch 46 finished \tANN training loss 2.238142\n",
            ">> Epoch 47 finished \tANN training loss 2.263890\n",
            ">> Epoch 48 finished \tANN training loss 2.307392\n",
            ">> Epoch 49 finished \tANN training loss 2.372641\n",
            ">> Epoch 50 finished \tANN training loss 2.248774\n",
            ">> Epoch 51 finished \tANN training loss 2.332099\n",
            ">> Epoch 52 finished \tANN training loss 2.296465\n",
            ">> Epoch 53 finished \tANN training loss 2.094367\n",
            ">> Epoch 54 finished \tANN training loss 2.199994\n",
            ">> Epoch 55 finished \tANN training loss 2.231629\n",
            ">> Epoch 56 finished \tANN training loss 2.304751\n",
            ">> Epoch 57 finished \tANN training loss 2.231750\n",
            ">> Epoch 58 finished \tANN training loss 2.124736\n",
            ">> Epoch 59 finished \tANN training loss 2.230936\n",
            ">> Epoch 60 finished \tANN training loss 2.141790\n",
            ">> Epoch 61 finished \tANN training loss 2.149280\n",
            ">> Epoch 62 finished \tANN training loss 2.339556\n",
            ">> Epoch 63 finished \tANN training loss 2.128143\n",
            ">> Epoch 64 finished \tANN training loss 2.125504\n",
            ">> Epoch 65 finished \tANN training loss 2.240109\n",
            ">> Epoch 66 finished \tANN training loss 2.336564\n",
            ">> Epoch 67 finished \tANN training loss 2.313321\n",
            ">> Epoch 68 finished \tANN training loss 2.183284\n",
            ">> Epoch 69 finished \tANN training loss 2.077144\n",
            ">> Epoch 70 finished \tANN training loss 2.143480\n",
            ">> Epoch 71 finished \tANN training loss 2.323889\n",
            ">> Epoch 72 finished \tANN training loss 2.105352\n",
            ">> Epoch 73 finished \tANN training loss 2.245227\n",
            ">> Epoch 74 finished \tANN training loss 2.327497\n",
            ">> Epoch 75 finished \tANN training loss 2.130037\n",
            ">> Epoch 76 finished \tANN training loss 2.065944\n",
            ">> Epoch 77 finished \tANN training loss 2.100027\n",
            ">> Epoch 78 finished \tANN training loss 2.120703\n",
            ">> Epoch 79 finished \tANN training loss 2.088842\n",
            ">> Epoch 80 finished \tANN training loss 2.207303\n",
            ">> Epoch 81 finished \tANN training loss 2.082068\n",
            ">> Epoch 82 finished \tANN training loss 1.953960\n",
            ">> Epoch 83 finished \tANN training loss 2.208924\n",
            ">> Epoch 84 finished \tANN training loss 1.983415\n",
            ">> Epoch 85 finished \tANN training loss 2.171996\n",
            ">> Epoch 86 finished \tANN training loss 1.981490\n",
            ">> Epoch 87 finished \tANN training loss 2.089919\n",
            ">> Epoch 88 finished \tANN training loss 2.003569\n",
            ">> Epoch 89 finished \tANN training loss 1.971410\n",
            ">> Epoch 90 finished \tANN training loss 2.010375\n",
            ">> Epoch 91 finished \tANN training loss 2.143923\n",
            ">> Epoch 92 finished \tANN training loss 2.079673\n",
            ">> Epoch 93 finished \tANN training loss 1.927127\n",
            ">> Epoch 94 finished \tANN training loss 1.968392\n",
            ">> Epoch 95 finished \tANN training loss 2.046866\n",
            ">> Epoch 96 finished \tANN training loss 1.967063\n",
            ">> Epoch 97 finished \tANN training loss 2.063042\n",
            ">> Epoch 98 finished \tANN training loss 2.164695\n",
            ">> Epoch 99 finished \tANN training loss 2.100575\n",
            ">> Epoch 100 finished \tANN training loss 2.050563\n",
            "[END] Fine tuning step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SupervisedDBNClassification()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SupervisedDBNClassification</label><div class=\"sk-toggleable__content\"><pre>SupervisedDBNClassification()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SupervisedDBNClassification()"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train model\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.01,\n",
        "                                         learning_rate=0.1,\n",
        "                                         n_epochs_rbm=10,\n",
        "                                         n_iter_backprop=100,\n",
        "                                         batch_size=32,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "\n",
        "# Fit model\n",
        "classifier.fit(X_train_res, y_train_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F46a3STOeJkU",
        "outputId": "91c77ed4-1736-47f6-fe2c-e91d0ae28204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Accuracy: 0.859259\n"
          ]
        }
      ],
      "source": [
        "# Test model\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXofFrM79Cx1"
      },
      "source": [
        "## **2nd Variation: CNV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyYgoinpMlGe"
      },
      "outputs": [],
      "source": [
        "# Read the datasets\n",
        "cnv = pd.read_csv('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Data/cnv_data_5000.csv')\n",
        "target = pd.read_csv('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Data/post_target.csv')\n",
        "\n",
        "# Loading dataset\n",
        "X = cnv.iloc[:, 1:]\n",
        "Y = target.iloc[:, 1]\n",
        "\n",
        "# Data scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQQLxd-8MQDi"
      },
      "source": [
        "### **Without SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6clraOAMMEk"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.01,\n",
        "                                         n_epochs_rbm=20,\n",
        "                                         n_iter_backprop=200,\n",
        "                                         batch_size=64,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "classifier.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx9RzHrAM7zL"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "from sklearn.metrics import accuracy_score\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa24MwhZNAw8"
      },
      "source": [
        "### **With SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdeYwg5dNAxI"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE on training data\n",
        "sm= SMOTE(k_neighbors=1,random_state=0)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, Y_train)\n",
        "\n",
        "print ('Shape of resampled data: {}'.format(X_train_res.shape))\n",
        "print ('Shape of Y: {}'.format(y_train_res.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwyyhnqzNAxI"
      },
      "outputs": [],
      "source": [
        "# Data visualization before SMOTE\n",
        "counter = Counter(Y_train)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Images/FS/preSMOTE_cnv.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp6B3PNyNMk0"
      },
      "outputs": [],
      "source": [
        "# Data visualization after SMOTE\n",
        "counter = Counter(y_train_res)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Images/FS/SMOTE_cnv.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRVwjPS0NOwI"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.01,\n",
        "                                         n_epochs_rbm=20,\n",
        "                                         n_iter_backprop=200,\n",
        "                                         batch_size=64,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "\n",
        "# Fit model\n",
        "classifier.fit(X_train_res, y_train_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxqgpRWvNRIm"
      },
      "outputs": [],
      "source": [
        "# Test model\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM9PcoM8NeDZ"
      },
      "source": [
        "## **3rd Variation: mRNA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suMvBPyeNeDu"
      },
      "outputs": [],
      "source": [
        "# Read the datasets\n",
        "mRNA = pd.read_csv('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Data/mRNA_data_5000.csv')\n",
        "target = pd.read_csv('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Data/post_target.csv')\n",
        "\n",
        "# Now you can proceed with your data preparation steps\n",
        "# Loading dataset\n",
        "X = mRNA.iloc[:, 1:]\n",
        "Y = target.iloc[:, 1]\n",
        "\n",
        "# Data scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j89Mt_rHNeDv"
      },
      "source": [
        "### **Without SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lit9lzhQNeDv"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "# from dbn.tensorflow import SupervisedDBNClassification\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.01,\n",
        "                                         n_epochs_rbm=20,\n",
        "                                         n_iter_backprop=200,\n",
        "                                         batch_size=64,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "classifier.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyLrUynTNeDw"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "from sklearn.metrics import accuracy_score\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9c_Wqu4NeDw"
      },
      "source": [
        "### **With SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldZVAXUFNeDx"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE on training data\n",
        "sm= SMOTE(k_neighbors=1,random_state=0)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, Y_train)\n",
        "\n",
        "print ('Shape of resampled data: {}'.format(X_train_res.shape))\n",
        "print ('Shape of Y: {}'.format(y_train_res.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wniA_5lGNeDx"
      },
      "outputs": [],
      "source": [
        "# Data visualization before SMOTE\n",
        "counter = Counter(Y_train)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes (DNA)')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Images/preSMOTE_mRNA.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGctay4ZNeDx"
      },
      "outputs": [],
      "source": [
        "# Data visualization after SMOTE\n",
        "counter = Counter(y_train_res)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes (DNA)')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/gdrive/My Drive/UTM Y4S2/BIOINFORMATICS MODELING AND SIMULATION/MEGA Project/Images/SMOTE_mRNA.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnbAHTFDNeDy"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.01,\n",
        "                                         n_epochs_rbm=20,\n",
        "                                         n_iter_backprop=200,\n",
        "                                         batch_size=64,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "\n",
        "# Fit model\n",
        "classifier.fit(X_train_res, y_train_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdVSyszONeDy"
      },
      "outputs": [],
      "source": [
        "# Test model\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxdhM640Xan"
      },
      "source": [
        "# **Single Omics Variation: miRNA + Copy Number Variation (CNV) + DNA_Methylation + mRNA**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__8ThBGW0dJL"
      },
      "source": [
        "## **Data Integration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYO_RSFP0enD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# miRNA_data = pd.read_csv('drive/MyDrive/BRCA_result/post_miRNA.csv', header=0, index_col=None)\n",
        "# cnv_data = pd.read_csv('drive/MyDrive/BRCA_result/post_cnv.csv', header=0, index_col=None)\n",
        "# mRNA_data = pd.read_csv('drive/MyDrive/BRCA_result/post_mRNA.csv', header=0, index_col=None)\n",
        "\n",
        "miRNA_data = pd.read_csv('/content/deep-belief-network/BRCA_result/post_miRNA.csv', header=0, index_col=None)\n",
        "cnv_data = pd.read_csv('/content/deep-belief-network/BRCA_result/post_cnv.csv', header=0, index_col=None)\n",
        "mRNA_data = pd.read_csv('/content/deep-belief-network/BRCA_result/post_mRNA.csv', header=0, index_col=None)\n",
        "\n",
        "cnv_data.rename(columns={'Unnamed: 0':'Sample'}, inplace=True)\n",
        "\n",
        "# Sort sample arrangement\n",
        "miRNA_data.sort_values(by='Sample', ascending=True, inplace=True)\n",
        "cnv_data.sort_values(by='Sample', ascending=True, inplace=True)\n",
        "mRNA_data .sort_values(by='Sample', ascending=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pffKqDI0g3A",
        "outputId": "9a48b017-083a-4dd5-e42d-f232551486ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(671, 38143)\n"
          ]
        }
      ],
      "source": [
        "# Merge all the datasets\n",
        "Merge_data = pd.merge(miRNA_data, cnv_data, on='Sample', how='inner', suffixes=('_miRNA', '_cnv'))\n",
        "Merge_data = pd.merge(Merge_data, mRNA_data, on='Sample', how='inner', suffixes=('', '_mRNA'))\n",
        "\n",
        "print(Merge_data.shape)\n",
        "# Print merged result into a CSV file\n",
        "Merge_data.to_csv('/content/deep-belief-network/BRCA_result/concat_data_3.csv', header=True, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvrpT_9K0jcb"
      },
      "source": [
        "## **Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DECRrdUJ07qq",
        "outputId": "efbbe775-3f1c-49f5-ee41-ded3087f3ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated Data's Row and Column Number :  (671, 38143)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Load data\n",
        "conct_data = pd.read_csv('/content/deep-belief-network/BRCA_result/concat_data_3.csv', header=0, index_col=None)\n",
        "\n",
        "# Get the row and column number of merged dataset\n",
        "print(\"Concatenated Data's Row and Column Number : \", conct_data.shape)\n",
        "\n",
        "# Get name of sample\n",
        "sample_name = conct_data['Sample'].tolist()\n",
        "\n",
        "# Get target value, y from sample class dataset\n",
        "sample_label = pd.read_csv('/content/deep-belief-network/BRCA_data/BRCA_label.csv',header=0,index_col=None)\n",
        "\n",
        "# Change label string to numerical value\n",
        "label_mapping ={'LumA': 0, 'LumB': 1, 'Basal': 2, 'Her2': 3, 'Normal': 4}\n",
        "sample_label['Label'] = sample_label['Label'].replace(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNk3Oyyq0-0s",
        "outputId": "4d1f1869-10a5-4972-ed9e-db35660a9a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features Importance: \n",
            "           Columns    Weight\n",
            "0         0_miRNA  0.000190\n",
            "1         1_miRNA  0.000142\n",
            "2         2_miRNA  0.000194\n",
            "3         3_miRNA -0.000257\n",
            "4         4_miRNA  0.000055\n",
            "...           ...       ...\n",
            "38137  18201_mRNA -0.000346\n",
            "38138  18202_mRNA -0.000362\n",
            "38139  18203_mRNA -0.000143\n",
            "38140  18204_mRNA -0.000382\n",
            "38141  18205_mRNA -0.000734\n",
            "\n",
            "[38142 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Get X and Y values\n",
        "X_omics = conct_data.iloc[:, 1:]\n",
        "Y_omics =  sample_label.iloc[:, 0]\n",
        "\n",
        "# Initialize an SVM model with a linear kernel\n",
        "estimator = SVR(kernel='linear')\n",
        "\n",
        "# Get the feature importance or weight\n",
        "estimator.fit(X_omics, Y_omics)\n",
        "features_importance = pd.DataFrame({'Columns': X_omics.columns, 'Weight':estimator.coef_.flatten()})\n",
        "print(\"Features Importance: \\n\",features_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "WqtQpiz31B-3",
        "outputId": "77ec7d62-a930-4fba-97d7-ddd979a4c286"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=SVR(kernel=&#x27;linear&#x27;), n_features_to_select=30000, step=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFE</label><div class=\"sk-toggleable__content\"><pre>RFE(estimator=SVR(kernel=&#x27;linear&#x27;), n_features_to_select=30000, step=100)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "RFE(estimator=SVR(kernel='linear'), n_features_to_select=30000, step=100)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply RFE to select the top 30000 features\n",
        "selector = RFE(estimator,n_features_to_select=30000, step=100)\n",
        "\n",
        "# Train model\n",
        "selector.fit(X_omics,Y_omics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxCINvtb1E5v",
        "outputId": "0e677ff3-24f5-4731-9a0c-641082578d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Selected Features: \n",
            "           Columns  Selected\n",
            "0         0_miRNA      True\n",
            "1         1_miRNA     False\n",
            "2         2_miRNA      True\n",
            "3         3_miRNA      True\n",
            "4         4_miRNA     False\n",
            "...           ...       ...\n",
            "38137  18201_mRNA      True\n",
            "38138  18202_mRNA      True\n",
            "38139  18203_mRNA     False\n",
            "38140  18204_mRNA      True\n",
            "38141  18205_mRNA      True\n",
            "\n",
            "[38142 rows x 2 columns]\n",
            "\n",
            "Features Ranking: \n",
            "           Columns  Ranking\n",
            "0         0_miRNA        1\n",
            "1         1_miRNA       25\n",
            "2         2_miRNA        1\n",
            "3         3_miRNA        1\n",
            "4         4_miRNA       61\n",
            "...           ...      ...\n",
            "38137  18201_mRNA        1\n",
            "38138  18202_mRNA        1\n",
            "38139  18203_mRNA       24\n",
            "38140  18204_mRNA        1\n",
            "38141  18205_mRNA        1\n",
            "\n",
            "[38142 rows x 2 columns]\n",
            "\n",
            "Unselected Features: \n",
            " Index(['1_miRNA', '4_miRNA', '10_miRNA', '17_miRNA', '20_miRNA', '23_miRNA',\n",
            "       '39_miRNA', '44_miRNA', '50_miRNA', '55_miRNA',\n",
            "       ...\n",
            "       '18118_mRNA', '18129_mRNA', '18135_mRNA', '18162_mRNA', '18164_mRNA',\n",
            "       '18166_mRNA', '18175_mRNA', '18196_mRNA', '18197_mRNA', '18203_mRNA'],\n",
            "      dtype='object', length=8142)\n"
          ]
        }
      ],
      "source": [
        " # Get selected features list\n",
        "features_selected = pd.DataFrame({'Columns':X_omics.columns, 'Selected':selector.support_})\n",
        "print(\"\\nSelected Features: \\n\",features_selected)\n",
        "\n",
        "# Get features ranking list\n",
        "features_rank = pd.DataFrame({'Columns': X_omics.columns, 'Ranking': selector.ranking_})\n",
        "print(\"\\nFeatures Ranking: \\n\",features_rank)\n",
        "\n",
        "# Get unselected features list\n",
        "features_unselected = X_omics.columns[np.logical_not(selector.get_support())]\n",
        "print(\"\\nUnselected Features: \\n\", features_unselected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_4ZicjR1Gbb",
        "outputId": "c9a437c3-3e09-402c-ed62-20eda60d56ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "selected feature from integrated data\n",
            "\n",
            "              Sample   0_miRNA   2_miRNA   3_miRNA   5_miRNA   6_miRNA  \\\n",
            "0    TCGA.3C.AAAU.01  0.068317  0.073899  0.524562 -0.038283  0.501125   \n",
            "1    TCGA.3C.AALI.01 -0.301684 -0.301310  0.419859  0.460975 -1.999304   \n",
            "2    TCGA.3C.AALJ.01 -0.150810 -0.126333 -0.958939  0.866585  2.074809   \n",
            "3    TCGA.3C.AALK.01  0.107831  0.095545  0.615389 -0.454282  0.227441   \n",
            "4    TCGA.5L.AAT0.01  0.395211  0.418441  0.500594 -1.545556 -0.952282   \n",
            "..               ...       ...       ...       ...       ...       ...   \n",
            "666  TCGA.WT.AB44.01  0.511958  0.496009  0.422382 -0.254668  0.528134   \n",
            "667  TCGA.XX.A899.01  1.225298  1.210610  0.050804 -1.238797  0.879213   \n",
            "668  TCGA.XX.A89A.01  0.667662  0.675998 -0.102655  0.305716  1.276369   \n",
            "669  TCGA.Z7.A8R5.01 -0.211878 -0.220135  0.177655 -0.716526 -0.598473   \n",
            "670  TCGA.Z7.A8R6.01  0.474240  0.481577 -0.078426  1.384395  1.237552   \n",
            "\n",
            "      7_miRNA   8_miRNA   9_miRNA  11_miRNA  ...  18193_mRNA  18194_mRNA  \\\n",
            "0   -2.390084 -2.406331 -0.760042  0.748550  ...   -0.337514   -1.562251   \n",
            "1   -0.659788 -0.651943 -1.050266 -0.957563  ...   -0.123063   -1.007943   \n",
            "2    1.080746  1.088164  0.523204 -0.664522  ...    1.125966    1.528513   \n",
            "3   -0.735552 -0.686623 -1.015715 -0.425881  ...   -0.045342   -0.331201   \n",
            "4   -0.280966 -0.233746 -0.818574  0.680191  ...    1.554868    0.552953   \n",
            "..        ...       ...       ...       ...  ...         ...         ...   \n",
            "666  0.757211  0.741367  0.378499  0.324924  ...   -0.184691   -0.533860   \n",
            "667  1.455206  1.460326  0.921697 -0.422295  ...   -0.077698    0.050713   \n",
            "668  1.253312  1.240593  0.556762 -1.081470  ...   -0.584793   -0.479839   \n",
            "669 -0.229774 -0.190906 -0.465549 -0.568303  ...   -0.385794   -0.069840   \n",
            "670  0.851405  0.851526  0.409391 -0.534668  ...    0.695443    1.215160   \n",
            "\n",
            "     18195_mRNA  18198_mRNA  18199_mRNA  18200_mRNA  18201_mRNA  18202_mRNA  \\\n",
            "0      0.432171    2.359316    2.242234    1.190506    1.753777    2.013674   \n",
            "1     -0.331534    0.513900    0.179770    1.588651    1.298863   -0.140439   \n",
            "2     -0.712248   -0.505798    0.658038   -1.008467    2.105007   -0.334864   \n",
            "3     -0.827854   -0.185209   -0.066780   -0.759323    0.916528   -0.278901   \n",
            "4     -0.005713    0.770811    1.161026    0.369677   -1.152811    0.075213   \n",
            "..          ...         ...         ...         ...         ...         ...   \n",
            "666   -0.589054   -1.275728   -1.285558   -0.987687   -0.044914   -2.221944   \n",
            "667   -0.234352    0.676426    1.285960    0.011981   -1.729619   -0.111461   \n",
            "668   -0.315246   -0.529672   -0.639022    0.963877    0.464810   -0.145830   \n",
            "669   -0.633829   -0.831864   -1.158951   -0.997635   -0.583245   -0.852285   \n",
            "670   -0.236672   -0.719424    0.018771    0.067254    0.691664   -0.082546   \n",
            "\n",
            "     18204_mRNA  18205_mRNA  \n",
            "0      1.214399    1.667340  \n",
            "1      0.302547   -1.162286  \n",
            "2     -0.379244   -0.547104  \n",
            "3     -0.939663    0.057166  \n",
            "4     -0.485591    1.268702  \n",
            "..          ...         ...  \n",
            "666   -1.128941   -1.289816  \n",
            "667    1.749823   -0.091739  \n",
            "668    0.586414    0.426581  \n",
            "669   -1.068255    0.430341  \n",
            "670   -1.364660   -0.720639  \n",
            "\n",
            "[671 rows x 30001 columns]\n"
          ]
        }
      ],
      "source": [
        "# Put selected features in dataframe with sample name\n",
        "selected_features = X_omics.iloc[:, selector.support_]\n",
        "pd_selected_features = pd.DataFrame(selected_features)\n",
        "pd_selected_features.insert(0, 'Sample', sample_name)\n",
        "print(\"\\nselected feature from integrated data\\n\")\n",
        "print(pd_selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWMhN-z1CtQ",
        "outputId": "a3b23cb4-e129-4c52-f929-c3085aafbc0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SVM-RFE Model Performance based on miRNA Data\n",
            "Coefficient of determination (R^2):  0.9931994180797747\n"
          ]
        }
      ],
      "source": [
        "# Test and evaluate model\n",
        "print(\"\\nSVM-RFE Model Performance based on concatenated Data\")\n",
        "print(\"Coefficient of determination (R^2): \",selector.score(X_omics,Y_omics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwuYDmjG1H1e",
        "outputId": "b3369665-7b65-4354-bffe-768a40dae748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success! Features Selection results can be seen in result folder.\n",
            "(671, 30001)\n"
          ]
        }
      ],
      "source": [
        "# Output merged result into a  CSV file\n",
        "pd_selected_features.to_csv('/content/deep-belief-network/BRCA_result/concat_data_30000.csv', header=True, index=False)\n",
        "print('Success! Features Selection results can be seen in result folder.')\n",
        "print(pd_selected_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oe2297SWMwg"
      },
      "source": [
        "## **Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Y_zDxraxQf",
        "outputId": "ea546280-6a9f-489b-e0d6-9c78c43a376f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-belief-network'...\n",
            "remote: Enumerating objects: 798, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 798 (delta 13), reused 20 (delta 9), pack-reused 763\u001b[K\n",
            "Receiving objects: 100% (798/798), 183.45 KiB | 3.33 MiB/s, done.\n",
            "Resolving deltas: 100% (459/459), done.\n",
            "/content/deep-belief-network\n",
            "Processing /content/deep-belief-network\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.16.4 (from deep-belief-network==1.0.5)\n",
            "  Downloading numpy-1.16.4.zip (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==0.18.1 (from deep-belief-network==1.0.5)\n",
            "  Downloading scipy-0.18.1.zip (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==0.18.1 (from deep-belief-network==1.0.5)\n",
            "  Downloading scikit-learn-0.18.1.tar.gz (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of deep-belief-network to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.5.0 (from deep-belief-network) (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.5.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install the package from GitHub\n",
        "!git clone https://github.com/albertbup/deep-belief-network.git\n",
        "%cd deep-belief-network\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PspEP51KN6T8",
        "outputId": "c266e9f0-b61f-4dba-d83e-ebbaa75b0b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucKlNnonWiUY",
        "outputId": "4dd4acae-cf6e-41e2-f2d9-717196346d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "concat data shape:  (671, 30001)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from dbn import SupervisedDBNClassification\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Set up seed\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "# Get the absolute path to the file\n",
        "file_path = os.path.abspath('/content/deep-belief-network/BRCA_result/concat_data_30000_latest.csv')\n",
        "labelFile_path = os.path.abspath('/content/deep-belief-network/BRCA_data/BRCA_label.csv')\n",
        "\n",
        "# Load dataset\n",
        "conct_data = pd.read_csv(file_path, header=0, index_col=None)\n",
        "print(\"concat data shape: \",conct_data.shape)\n",
        "# Get target value, y from sample class dataset\n",
        "sample_label = pd.read_csv(labelFile_path,header=0,index_col=None)\n",
        "\n",
        "# Change label string to numerical value\n",
        "label_mapping ={'LumA': 0, 'LumB': 1, 'Basal': 2, 'Her2': 3, 'Normal': 4}\n",
        "sample_label['Label'] = sample_label['Label'].replace(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngF4IwObZeW9"
      },
      "outputs": [],
      "source": [
        "# Get X and Y values\n",
        "X = conct_data.iloc[:, 1:]\n",
        "Y  =  sample_label.iloc[:, 0]\n",
        "\n",
        "# Normalize the input data to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train_cont, X_test_cont, Y_train_cont, Y_test_cont = train_test_split(X_scaled, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHdJ-CEtWl3c"
      },
      "source": [
        "### **Without SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AGBCZVETZl-o",
        "outputId": "ac102362-e327-440b-e78d-436c447f8bab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] Pre-training step:\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 2734896973753.649414\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 11 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 12 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 13 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 14 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 15 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 16 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 17 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 18 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 19 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 20 finished \tRBM Reconstruction error 6062.290785\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 0.003179\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 0.000556\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 0.000007\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 11 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 12 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 13 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 14 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 15 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 16 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 17 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 18 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 19 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 20 finished \tRBM Reconstruction error 0.000000\n",
            "[END] Pre-training step\n",
            "[START] Fine tuning step:\n",
            ">> Epoch 1 finished \tANN training loss 7.937658\n",
            ">> Epoch 2 finished \tANN training loss 7.839233\n",
            ">> Epoch 3 finished \tANN training loss 7.748833\n",
            ">> Epoch 4 finished \tANN training loss 7.629974\n",
            ">> Epoch 5 finished \tANN training loss 7.388118\n",
            ">> Epoch 6 finished \tANN training loss 6.941667\n",
            ">> Epoch 7 finished \tANN training loss 6.654340\n",
            ">> Epoch 8 finished \tANN training loss 6.541574\n",
            ">> Epoch 9 finished \tANN training loss 6.464660\n",
            ">> Epoch 10 finished \tANN training loss 6.401394\n",
            ">> Epoch 11 finished \tANN training loss 6.444496\n",
            ">> Epoch 12 finished \tANN training loss 6.436055\n",
            ">> Epoch 13 finished \tANN training loss 6.441330\n",
            ">> Epoch 14 finished \tANN training loss 6.420951\n",
            ">> Epoch 15 finished \tANN training loss 6.430440\n",
            ">> Epoch 16 finished \tANN training loss 6.417884\n",
            ">> Epoch 17 finished \tANN training loss 6.336316\n",
            ">> Epoch 18 finished \tANN training loss 6.294969\n",
            ">> Epoch 19 finished \tANN training loss 6.297052\n",
            ">> Epoch 20 finished \tANN training loss 6.278100\n",
            ">> Epoch 21 finished \tANN training loss 6.236149\n",
            ">> Epoch 22 finished \tANN training loss 6.242525\n",
            ">> Epoch 23 finished \tANN training loss 6.193282\n",
            ">> Epoch 24 finished \tANN training loss 6.166067\n",
            ">> Epoch 25 finished \tANN training loss 6.119251\n",
            ">> Epoch 26 finished \tANN training loss 6.036650\n",
            ">> Epoch 27 finished \tANN training loss 6.032312\n",
            ">> Epoch 28 finished \tANN training loss 5.911020\n",
            ">> Epoch 29 finished \tANN training loss 5.881837\n",
            ">> Epoch 30 finished \tANN training loss 5.750628\n",
            ">> Epoch 31 finished \tANN training loss 5.714543\n",
            ">> Epoch 32 finished \tANN training loss 5.651284\n",
            ">> Epoch 33 finished \tANN training loss 5.538156\n",
            ">> Epoch 34 finished \tANN training loss 5.448248\n",
            ">> Epoch 35 finished \tANN training loss 5.432601\n",
            ">> Epoch 36 finished \tANN training loss 5.401697\n",
            ">> Epoch 37 finished \tANN training loss 5.235996\n",
            ">> Epoch 38 finished \tANN training loss 5.163460\n",
            ">> Epoch 39 finished \tANN training loss 5.144847\n",
            ">> Epoch 40 finished \tANN training loss 5.188204\n",
            ">> Epoch 41 finished \tANN training loss 5.032668\n",
            ">> Epoch 42 finished \tANN training loss 4.988630\n",
            ">> Epoch 43 finished \tANN training loss 4.889377\n",
            ">> Epoch 44 finished \tANN training loss 4.808824\n",
            ">> Epoch 45 finished \tANN training loss 4.793472\n",
            ">> Epoch 46 finished \tANN training loss 4.671167\n",
            ">> Epoch 47 finished \tANN training loss 4.696872\n",
            ">> Epoch 48 finished \tANN training loss 4.567574\n",
            ">> Epoch 49 finished \tANN training loss 4.520824\n",
            ">> Epoch 50 finished \tANN training loss 4.482856\n",
            ">> Epoch 51 finished \tANN training loss 4.378250\n",
            ">> Epoch 52 finished \tANN training loss 4.489329\n",
            ">> Epoch 53 finished \tANN training loss 4.388109\n",
            ">> Epoch 54 finished \tANN training loss 4.408732\n",
            ">> Epoch 55 finished \tANN training loss 4.331966\n",
            ">> Epoch 56 finished \tANN training loss 4.315128\n",
            ">> Epoch 57 finished \tANN training loss 4.283541\n",
            ">> Epoch 58 finished \tANN training loss 4.191347\n",
            ">> Epoch 59 finished \tANN training loss 4.194877\n",
            ">> Epoch 60 finished \tANN training loss 4.079094\n",
            ">> Epoch 61 finished \tANN training loss 4.049132\n",
            ">> Epoch 62 finished \tANN training loss 4.036929\n",
            ">> Epoch 63 finished \tANN training loss 3.919137\n",
            ">> Epoch 64 finished \tANN training loss 3.997703\n",
            ">> Epoch 65 finished \tANN training loss 3.945590\n",
            ">> Epoch 66 finished \tANN training loss 3.956964\n",
            ">> Epoch 67 finished \tANN training loss 3.845494\n",
            ">> Epoch 68 finished \tANN training loss 3.900768\n",
            ">> Epoch 69 finished \tANN training loss 3.837928\n",
            ">> Epoch 70 finished \tANN training loss 3.841628\n",
            ">> Epoch 71 finished \tANN training loss 3.834948\n",
            ">> Epoch 72 finished \tANN training loss 3.659409\n",
            ">> Epoch 73 finished \tANN training loss 3.837192\n",
            ">> Epoch 74 finished \tANN training loss 3.618358\n",
            ">> Epoch 75 finished \tANN training loss 3.707370\n",
            ">> Epoch 76 finished \tANN training loss 3.669862\n",
            ">> Epoch 77 finished \tANN training loss 3.683208\n",
            ">> Epoch 78 finished \tANN training loss 3.611038\n",
            ">> Epoch 79 finished \tANN training loss 3.556759\n",
            ">> Epoch 80 finished \tANN training loss 3.526279\n",
            ">> Epoch 81 finished \tANN training loss 3.563947\n",
            ">> Epoch 82 finished \tANN training loss 3.513809\n",
            ">> Epoch 83 finished \tANN training loss 3.504407\n",
            ">> Epoch 84 finished \tANN training loss 3.485261\n",
            ">> Epoch 85 finished \tANN training loss 3.393082\n",
            ">> Epoch 86 finished \tANN training loss 3.440801\n",
            ">> Epoch 87 finished \tANN training loss 3.496421\n",
            ">> Epoch 88 finished \tANN training loss 3.353072\n",
            ">> Epoch 89 finished \tANN training loss 3.358807\n",
            ">> Epoch 90 finished \tANN training loss 3.201940\n",
            ">> Epoch 91 finished \tANN training loss 3.292918\n",
            ">> Epoch 92 finished \tANN training loss 3.219713\n",
            ">> Epoch 93 finished \tANN training loss 3.245200\n",
            ">> Epoch 94 finished \tANN training loss 3.198786\n",
            ">> Epoch 95 finished \tANN training loss 3.186709\n",
            ">> Epoch 96 finished \tANN training loss 3.192911\n",
            ">> Epoch 97 finished \tANN training loss 3.026355\n",
            ">> Epoch 98 finished \tANN training loss 3.171378\n",
            ">> Epoch 99 finished \tANN training loss 3.105964\n",
            ">> Epoch 100 finished \tANN training loss 3.049247\n",
            ">> Epoch 101 finished \tANN training loss 2.956509\n",
            ">> Epoch 102 finished \tANN training loss 2.960326\n",
            ">> Epoch 103 finished \tANN training loss 2.880888\n",
            ">> Epoch 104 finished \tANN training loss 2.851921\n",
            ">> Epoch 105 finished \tANN training loss 2.875165\n",
            ">> Epoch 106 finished \tANN training loss 2.835207\n",
            ">> Epoch 107 finished \tANN training loss 2.762238\n",
            ">> Epoch 108 finished \tANN training loss 2.857859\n",
            ">> Epoch 109 finished \tANN training loss 2.833576\n",
            ">> Epoch 110 finished \tANN training loss 2.675876\n",
            ">> Epoch 111 finished \tANN training loss 2.736323\n",
            ">> Epoch 112 finished \tANN training loss 2.883500\n",
            ">> Epoch 113 finished \tANN training loss 2.643236\n",
            ">> Epoch 114 finished \tANN training loss 2.663938\n",
            ">> Epoch 115 finished \tANN training loss 2.698299\n",
            ">> Epoch 116 finished \tANN training loss 2.522902\n",
            ">> Epoch 117 finished \tANN training loss 2.441014\n",
            ">> Epoch 118 finished \tANN training loss 2.543773\n",
            ">> Epoch 119 finished \tANN training loss 2.495694\n",
            ">> Epoch 120 finished \tANN training loss 2.600243\n",
            ">> Epoch 121 finished \tANN training loss 2.407590\n",
            ">> Epoch 122 finished \tANN training loss 2.526391\n",
            ">> Epoch 123 finished \tANN training loss 2.473225\n",
            ">> Epoch 124 finished \tANN training loss 2.578847\n",
            ">> Epoch 125 finished \tANN training loss 2.408824\n",
            ">> Epoch 126 finished \tANN training loss 2.426800\n",
            ">> Epoch 127 finished \tANN training loss 2.122580\n",
            ">> Epoch 128 finished \tANN training loss 2.475189\n",
            ">> Epoch 129 finished \tANN training loss 2.332478\n",
            ">> Epoch 130 finished \tANN training loss 2.371669\n",
            ">> Epoch 131 finished \tANN training loss 2.344606\n",
            ">> Epoch 132 finished \tANN training loss 2.270758\n",
            ">> Epoch 133 finished \tANN training loss 2.176973\n",
            ">> Epoch 134 finished \tANN training loss 2.437831\n",
            ">> Epoch 135 finished \tANN training loss 2.198456\n",
            ">> Epoch 136 finished \tANN training loss 2.580419\n",
            ">> Epoch 137 finished \tANN training loss 2.319526\n",
            ">> Epoch 138 finished \tANN training loss 2.026508\n",
            ">> Epoch 139 finished \tANN training loss 2.297939\n",
            ">> Epoch 140 finished \tANN training loss 2.047078\n",
            ">> Epoch 141 finished \tANN training loss 2.046108\n",
            ">> Epoch 142 finished \tANN training loss 2.178200\n",
            ">> Epoch 143 finished \tANN training loss 2.015705\n",
            ">> Epoch 144 finished \tANN training loss 2.409951\n",
            ">> Epoch 145 finished \tANN training loss 2.219848\n",
            ">> Epoch 146 finished \tANN training loss 1.909542\n",
            ">> Epoch 147 finished \tANN training loss 2.000260\n",
            ">> Epoch 148 finished \tANN training loss 1.903570\n",
            ">> Epoch 149 finished \tANN training loss 1.839064\n",
            ">> Epoch 150 finished \tANN training loss 1.838419\n",
            ">> Epoch 151 finished \tANN training loss 1.909302\n",
            ">> Epoch 152 finished \tANN training loss 1.852859\n",
            ">> Epoch 153 finished \tANN training loss 1.712292\n",
            ">> Epoch 154 finished \tANN training loss 1.819205\n",
            ">> Epoch 155 finished \tANN training loss 1.839136\n",
            ">> Epoch 156 finished \tANN training loss 1.846876\n",
            ">> Epoch 157 finished \tANN training loss 2.249301\n",
            ">> Epoch 158 finished \tANN training loss 2.062550\n",
            ">> Epoch 159 finished \tANN training loss 2.254284\n",
            ">> Epoch 160 finished \tANN training loss 1.825565\n",
            ">> Epoch 161 finished \tANN training loss 1.778756\n",
            ">> Epoch 162 finished \tANN training loss 1.857068\n",
            ">> Epoch 163 finished \tANN training loss 1.720858\n",
            ">> Epoch 164 finished \tANN training loss 1.761718\n",
            ">> Epoch 165 finished \tANN training loss 1.945855\n",
            ">> Epoch 166 finished \tANN training loss 2.094815\n",
            ">> Epoch 167 finished \tANN training loss 1.501373\n",
            ">> Epoch 168 finished \tANN training loss 1.761218\n",
            ">> Epoch 169 finished \tANN training loss 2.138649\n",
            ">> Epoch 170 finished \tANN training loss 1.709556\n",
            ">> Epoch 171 finished \tANN training loss 1.594520\n",
            ">> Epoch 172 finished \tANN training loss 1.603594\n",
            ">> Epoch 173 finished \tANN training loss 1.555252\n",
            ">> Epoch 174 finished \tANN training loss 1.792582\n",
            ">> Epoch 175 finished \tANN training loss 1.705201\n",
            ">> Epoch 176 finished \tANN training loss 1.508436\n",
            ">> Epoch 177 finished \tANN training loss 1.545376\n",
            ">> Epoch 178 finished \tANN training loss 1.628072\n",
            ">> Epoch 179 finished \tANN training loss 2.087276\n",
            ">> Epoch 180 finished \tANN training loss 1.570822\n",
            ">> Epoch 181 finished \tANN training loss 1.474562\n",
            ">> Epoch 182 finished \tANN training loss 1.948526\n",
            ">> Epoch 183 finished \tANN training loss 1.621468\n",
            ">> Epoch 184 finished \tANN training loss 2.270592\n",
            ">> Epoch 185 finished \tANN training loss 1.921666\n",
            ">> Epoch 186 finished \tANN training loss 1.481689\n",
            ">> Epoch 187 finished \tANN training loss 1.507266\n",
            ">> Epoch 188 finished \tANN training loss 1.368288\n",
            ">> Epoch 189 finished \tANN training loss 1.352443\n",
            ">> Epoch 190 finished \tANN training loss 1.286044\n",
            ">> Epoch 191 finished \tANN training loss 1.294579\n",
            ">> Epoch 192 finished \tANN training loss 1.478128\n",
            ">> Epoch 193 finished \tANN training loss 1.295667\n",
            ">> Epoch 194 finished \tANN training loss 1.368963\n",
            ">> Epoch 195 finished \tANN training loss 1.700751\n",
            ">> Epoch 196 finished \tANN training loss 1.488626\n",
            ">> Epoch 197 finished \tANN training loss 1.382967\n",
            ">> Epoch 198 finished \tANN training loss 1.600176\n",
            ">> Epoch 199 finished \tANN training loss 1.364303\n",
            ">> Epoch 200 finished \tANN training loss 1.198222\n",
            "[END] Fine tuning step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SupervisedDBNClassification()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SupervisedDBNClassification</label><div class=\"sk-toggleable__content\"><pre>SupervisedDBNClassification()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SupervisedDBNClassification()"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train model\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.01,\n",
        "                                         n_epochs_rbm=20,\n",
        "                                         n_iter_backprop=200,\n",
        "                                         batch_size=64,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "\n",
        "# Fit model\n",
        "classifier.fit(X_train_cont, Y_train_cont)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9m184jkZn3_",
        "outputId": "aceab7c2-c125-47ed-dcf8-fb59245b4144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Accuracy: 0.874074\n"
          ]
        }
      ],
      "source": [
        "# Test model\n",
        "Y_pred = classifier.predict(X_test_cont)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test_cont, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c4FXSAPWuBo"
      },
      "source": [
        "### **SMOTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7oepKUBaB-Q",
        "outputId": "849e2e05-79e7-4391-86c6-74d002df0bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of resampled data: (1385, 30000)\n",
            "Shape of Y: (1385,)\n"
          ]
        }
      ],
      "source": [
        "# Apply SMOTE on training data\n",
        "sm= SMOTE(k_neighbors=1,random_state=0)\n",
        "X_train_cont_res, y_train_cont_res = sm.fit_resample(X_train_cont, Y_train_cont)\n",
        "\n",
        "print ('Shape of resampled data: {}'.format(X_train_cont_res.shape))\n",
        "print ('Shape of Y: {}'.format(y_train_cont_res.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv7TFgGdaIwF"
      },
      "outputs": [],
      "source": [
        "# Data visualization before SMOTE\n",
        "counter = Counter(Y_train_cont)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/deep-belief-network/BRCA_result/static/images/preSMOTE_conct.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP9Io8IdaKZi"
      },
      "outputs": [],
      "source": [
        "# Data visualization after SMOTE\n",
        "counter = Counter(y_train_cont_res)\n",
        "label, values = zip(*counter.items())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.bar(label, values)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Classes')\n",
        "\n",
        "# Display the plot\n",
        "plt.savefig('/content/deep-belief-network/BRCA_result/static/images/SMOTE_conct.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rkGkEqIaEov",
        "outputId": "ec15091d-107a-49cc-c6b6-d331c7567e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] Pre-training step:\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 201047959323019018240.000000\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 11 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 12 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 13 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 14 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 15 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 16 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 17 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 18 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 19 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 20 finished \tRBM Reconstruction error 5902.945716\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 0.000073\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 11 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 12 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 13 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 14 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 15 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 16 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 17 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 18 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 19 finished \tRBM Reconstruction error 0.000000\n",
            ">> Epoch 20 finished \tRBM Reconstruction error 0.000000\n",
            "[END] Pre-training step\n",
            "[START] Fine tuning step:\n",
            ">> Epoch 1 finished \tANN training loss 8.061769\n",
            ">> Epoch 2 finished \tANN training loss 8.059229\n",
            ">> Epoch 3 finished \tANN training loss 8.055051\n",
            ">> Epoch 4 finished \tANN training loss 8.052186\n",
            ">> Epoch 5 finished \tANN training loss 8.045177\n",
            ">> Epoch 6 finished \tANN training loss 8.036635\n",
            ">> Epoch 7 finished \tANN training loss 8.024153\n",
            ">> Epoch 8 finished \tANN training loss 8.005396\n",
            ">> Epoch 9 finished \tANN training loss 7.981412\n",
            ">> Epoch 10 finished \tANN training loss 7.940990\n",
            ">> Epoch 11 finished \tANN training loss 7.891113\n",
            ">> Epoch 12 finished \tANN training loss 7.814459\n",
            ">> Epoch 13 finished \tANN training loss 7.713849\n",
            ">> Epoch 14 finished \tANN training loss 7.549241\n",
            ">> Epoch 15 finished \tANN training loss 7.335804\n",
            ">> Epoch 16 finished \tANN training loss 7.036209\n",
            ">> Epoch 17 finished \tANN training loss 6.685619\n",
            ">> Epoch 18 finished \tANN training loss 6.280746\n",
            ">> Epoch 19 finished \tANN training loss 5.895117\n",
            ">> Epoch 20 finished \tANN training loss 5.461234\n",
            ">> Epoch 21 finished \tANN training loss 5.114024\n",
            ">> Epoch 22 finished \tANN training loss 4.744852\n",
            ">> Epoch 23 finished \tANN training loss 4.514504\n",
            ">> Epoch 24 finished \tANN training loss 4.233447\n",
            ">> Epoch 25 finished \tANN training loss 3.974253\n",
            ">> Epoch 26 finished \tANN training loss 3.718076\n",
            ">> Epoch 27 finished \tANN training loss 3.611463\n",
            ">> Epoch 28 finished \tANN training loss 3.363876\n",
            ">> Epoch 29 finished \tANN training loss 3.306180\n",
            ">> Epoch 30 finished \tANN training loss 2.971018\n",
            ">> Epoch 31 finished \tANN training loss 2.895342\n",
            ">> Epoch 32 finished \tANN training loss 2.792755\n",
            ">> Epoch 33 finished \tANN training loss 2.514464\n",
            ">> Epoch 34 finished \tANN training loss 2.529999\n",
            ">> Epoch 35 finished \tANN training loss 2.474001\n",
            ">> Epoch 36 finished \tANN training loss 2.328409\n",
            ">> Epoch 37 finished \tANN training loss 2.215248\n",
            ">> Epoch 38 finished \tANN training loss 2.179265\n",
            ">> Epoch 39 finished \tANN training loss 2.143083\n",
            ">> Epoch 40 finished \tANN training loss 2.073677\n",
            ">> Epoch 41 finished \tANN training loss 1.952116\n",
            ">> Epoch 42 finished \tANN training loss 1.917427\n",
            ">> Epoch 43 finished \tANN training loss 1.844432\n",
            ">> Epoch 44 finished \tANN training loss 1.954554\n",
            ">> Epoch 45 finished \tANN training loss 1.716274\n",
            ">> Epoch 46 finished \tANN training loss 1.664104\n",
            ">> Epoch 47 finished \tANN training loss 1.671324\n",
            ">> Epoch 48 finished \tANN training loss 1.567750\n",
            ">> Epoch 49 finished \tANN training loss 1.526717\n",
            ">> Epoch 50 finished \tANN training loss 1.580125\n",
            ">> Epoch 51 finished \tANN training loss 1.517212\n",
            ">> Epoch 52 finished \tANN training loss 1.474499\n",
            ">> Epoch 53 finished \tANN training loss 1.396406\n",
            ">> Epoch 54 finished \tANN training loss 1.420660\n",
            ">> Epoch 55 finished \tANN training loss 1.414244\n",
            ">> Epoch 56 finished \tANN training loss 1.414523\n",
            ">> Epoch 57 finished \tANN training loss 1.253417\n",
            ">> Epoch 58 finished \tANN training loss 1.384670\n",
            ">> Epoch 59 finished \tANN training loss 1.303135\n",
            ">> Epoch 60 finished \tANN training loss 1.269944\n",
            ">> Epoch 61 finished \tANN training loss 1.288956\n",
            ">> Epoch 62 finished \tANN training loss 1.189961\n",
            ">> Epoch 63 finished \tANN training loss 1.113698\n",
            ">> Epoch 64 finished \tANN training loss 1.147401\n",
            ">> Epoch 65 finished \tANN training loss 1.172257\n",
            ">> Epoch 66 finished \tANN training loss 1.179242\n",
            ">> Epoch 67 finished \tANN training loss 1.097331\n",
            ">> Epoch 68 finished \tANN training loss 1.071641\n",
            ">> Epoch 69 finished \tANN training loss 1.104160\n",
            ">> Epoch 70 finished \tANN training loss 0.959720\n",
            ">> Epoch 71 finished \tANN training loss 1.023223\n",
            ">> Epoch 72 finished \tANN training loss 1.034448\n",
            ">> Epoch 73 finished \tANN training loss 1.036599\n",
            ">> Epoch 74 finished \tANN training loss 1.046868\n",
            ">> Epoch 75 finished \tANN training loss 1.019260\n",
            ">> Epoch 76 finished \tANN training loss 0.872242\n",
            ">> Epoch 77 finished \tANN training loss 0.923963\n",
            ">> Epoch 78 finished \tANN training loss 1.099646\n",
            ">> Epoch 79 finished \tANN training loss 0.915029\n",
            ">> Epoch 80 finished \tANN training loss 0.971427\n",
            ">> Epoch 81 finished \tANN training loss 0.914372\n",
            ">> Epoch 82 finished \tANN training loss 1.094732\n",
            ">> Epoch 83 finished \tANN training loss 0.894819\n",
            ">> Epoch 84 finished \tANN training loss 0.778788\n",
            ">> Epoch 85 finished \tANN training loss 0.847547\n",
            ">> Epoch 86 finished \tANN training loss 0.778272\n",
            ">> Epoch 87 finished \tANN training loss 0.802010\n",
            ">> Epoch 88 finished \tANN training loss 0.907890\n",
            ">> Epoch 89 finished \tANN training loss 0.904677\n",
            ">> Epoch 90 finished \tANN training loss 0.793237\n",
            ">> Epoch 91 finished \tANN training loss 0.813517\n",
            ">> Epoch 92 finished \tANN training loss 0.759825\n",
            ">> Epoch 93 finished \tANN training loss 0.728091\n",
            ">> Epoch 94 finished \tANN training loss 0.702522\n",
            ">> Epoch 95 finished \tANN training loss 0.705512\n",
            ">> Epoch 96 finished \tANN training loss 0.708794\n",
            ">> Epoch 97 finished \tANN training loss 0.667063\n",
            ">> Epoch 98 finished \tANN training loss 0.698320\n",
            ">> Epoch 99 finished \tANN training loss 0.735175\n",
            ">> Epoch 100 finished \tANN training loss 0.684127\n",
            ">> Epoch 101 finished \tANN training loss 0.716412\n",
            ">> Epoch 102 finished \tANN training loss 0.638370\n",
            ">> Epoch 103 finished \tANN training loss 0.665082\n",
            ">> Epoch 104 finished \tANN training loss 0.644413\n",
            ">> Epoch 105 finished \tANN training loss 0.688039\n",
            ">> Epoch 106 finished \tANN training loss 0.629854\n",
            ">> Epoch 107 finished \tANN training loss 0.642184\n",
            ">> Epoch 108 finished \tANN training loss 0.609723\n",
            ">> Epoch 109 finished \tANN training loss 0.622529\n",
            ">> Epoch 110 finished \tANN training loss 0.637814\n",
            ">> Epoch 111 finished \tANN training loss 0.584054\n",
            ">> Epoch 112 finished \tANN training loss 0.624123\n",
            ">> Epoch 113 finished \tANN training loss 0.559043\n",
            ">> Epoch 114 finished \tANN training loss 0.555023\n",
            ">> Epoch 115 finished \tANN training loss 0.797247\n",
            ">> Epoch 116 finished \tANN training loss 0.594317\n",
            ">> Epoch 117 finished \tANN training loss 0.707490\n",
            ">> Epoch 118 finished \tANN training loss 0.618494\n",
            ">> Epoch 119 finished \tANN training loss 0.552617\n",
            ">> Epoch 120 finished \tANN training loss 0.545376\n",
            ">> Epoch 121 finished \tANN training loss 0.499089\n",
            ">> Epoch 122 finished \tANN training loss 0.640224\n",
            ">> Epoch 123 finished \tANN training loss 0.530564\n",
            ">> Epoch 124 finished \tANN training loss 0.510495\n",
            ">> Epoch 125 finished \tANN training loss 0.490275\n",
            ">> Epoch 126 finished \tANN training loss 0.450307\n",
            ">> Epoch 127 finished \tANN training loss 0.473057\n",
            ">> Epoch 128 finished \tANN training loss 0.588278\n",
            ">> Epoch 129 finished \tANN training loss 0.443518\n",
            ">> Epoch 130 finished \tANN training loss 0.540228\n",
            ">> Epoch 131 finished \tANN training loss 0.549813\n",
            ">> Epoch 132 finished \tANN training loss 0.472621\n",
            ">> Epoch 133 finished \tANN training loss 0.445339\n",
            ">> Epoch 134 finished \tANN training loss 0.477691\n",
            ">> Epoch 135 finished \tANN training loss 0.479432\n",
            ">> Epoch 136 finished \tANN training loss 0.493809\n",
            ">> Epoch 137 finished \tANN training loss 0.467462\n",
            ">> Epoch 138 finished \tANN training loss 0.407332\n",
            ">> Epoch 139 finished \tANN training loss 0.488048\n",
            ">> Epoch 140 finished \tANN training loss 0.446284\n",
            ">> Epoch 141 finished \tANN training loss 0.381417\n",
            ">> Epoch 142 finished \tANN training loss 0.527691\n",
            ">> Epoch 143 finished \tANN training loss 0.413036\n",
            ">> Epoch 144 finished \tANN training loss 0.365490\n",
            ">> Epoch 145 finished \tANN training loss 0.386452\n",
            ">> Epoch 146 finished \tANN training loss 0.365279\n",
            ">> Epoch 147 finished \tANN training loss 0.368788\n",
            ">> Epoch 148 finished \tANN training loss 0.384522\n",
            ">> Epoch 149 finished \tANN training loss 0.345359\n",
            ">> Epoch 150 finished \tANN training loss 0.421599\n",
            ">> Epoch 151 finished \tANN training loss 0.350701\n",
            ">> Epoch 152 finished \tANN training loss 0.354396\n",
            ">> Epoch 153 finished \tANN training loss 0.344039\n",
            ">> Epoch 154 finished \tANN training loss 0.357558\n",
            ">> Epoch 155 finished \tANN training loss 0.354849\n",
            ">> Epoch 156 finished \tANN training loss 0.335759\n",
            ">> Epoch 157 finished \tANN training loss 0.333969\n",
            ">> Epoch 158 finished \tANN training loss 0.375403\n",
            ">> Epoch 159 finished \tANN training loss 0.337278\n",
            ">> Epoch 160 finished \tANN training loss 0.342398\n",
            ">> Epoch 161 finished \tANN training loss 0.369976\n",
            ">> Epoch 162 finished \tANN training loss 0.300667\n",
            ">> Epoch 163 finished \tANN training loss 0.481448\n",
            ">> Epoch 164 finished \tANN training loss 0.349064\n",
            ">> Epoch 165 finished \tANN training loss 0.315138\n",
            ">> Epoch 166 finished \tANN training loss 0.365837\n",
            ">> Epoch 167 finished \tANN training loss 0.279381\n",
            ">> Epoch 168 finished \tANN training loss 0.377125\n",
            ">> Epoch 169 finished \tANN training loss 0.278681\n",
            ">> Epoch 170 finished \tANN training loss 0.317689\n",
            ">> Epoch 171 finished \tANN training loss 0.294808\n",
            ">> Epoch 172 finished \tANN training loss 0.333648\n",
            ">> Epoch 173 finished \tANN training loss 0.326910\n",
            ">> Epoch 174 finished \tANN training loss 0.255889\n",
            ">> Epoch 175 finished \tANN training loss 0.269880\n",
            ">> Epoch 176 finished \tANN training loss 0.279955\n",
            ">> Epoch 177 finished \tANN training loss 0.333919\n",
            ">> Epoch 178 finished \tANN training loss 0.287695\n",
            ">> Epoch 179 finished \tANN training loss 0.296868\n",
            ">> Epoch 180 finished \tANN training loss 0.292597\n",
            ">> Epoch 181 finished \tANN training loss 0.230436\n",
            ">> Epoch 182 finished \tANN training loss 0.256442\n",
            ">> Epoch 183 finished \tANN training loss 0.251474\n",
            ">> Epoch 184 finished \tANN training loss 0.309425\n",
            ">> Epoch 185 finished \tANN training loss 0.268700\n",
            ">> Epoch 186 finished \tANN training loss 0.259641\n",
            ">> Epoch 187 finished \tANN training loss 0.295068\n",
            ">> Epoch 188 finished \tANN training loss 0.282475\n",
            ">> Epoch 189 finished \tANN training loss 0.228287\n",
            ">> Epoch 190 finished \tANN training loss 0.288685\n",
            ">> Epoch 191 finished \tANN training loss 0.226810\n",
            ">> Epoch 192 finished \tANN training loss 0.253359\n",
            ">> Epoch 193 finished \tANN training loss 0.292588\n",
            ">> Epoch 194 finished \tANN training loss 0.254419\n",
            ">> Epoch 195 finished \tANN training loss 0.270672\n",
            ">> Epoch 196 finished \tANN training loss 0.241318\n",
            ">> Epoch 197 finished \tANN training loss 0.217415\n",
            ">> Epoch 198 finished \tANN training loss 0.260819\n",
            ">> Epoch 199 finished \tANN training loss 0.212884\n",
            ">> Epoch 200 finished \tANN training loss 0.248525\n",
            "[END] Fine tuning step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SupervisedDBNClassification()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SupervisedDBNClassification</label><div class=\"sk-toggleable__content\"><pre>SupervisedDBNClassification()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SupervisedDBNClassification()"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train model\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.01,\n",
        "                                         n_epochs_rbm=20,\n",
        "                                         n_iter_backprop=200,\n",
        "                                         batch_size=64,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "\n",
        "# Fit model\n",
        "classifier.fit(X_train_cont_res, y_train_cont_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZr5l-3SaF_f",
        "outputId": "baf02a96-67f4-4af9-e2e4-081f4812b93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Accuracy: 0.874074\n"
          ]
        }
      ],
      "source": [
        "# Test model\n",
        "Y_pred = classifier.predict(X_test_cont)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test_cont, Y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
